# bili_perfect_crawler.py
import requests
import time
import os
import pandas as pd
import urllib.parse
import random
from tqdm import tqdm

def crawl_perfect(target_total=10000, pages_per_keyword=20):
    """
    å®Œç¾ç‰ˆBç«™çˆ¬è™«ï¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰URLæ ¼å¼ï¼Œå½»åº•è§£å†³'cover_url'é—®é¢˜
    """
    
    # ==================== æ ¸å¿ƒé…ç½®åŒºï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰ ====================
    COOKIE = 'buvid3=B1EDF9ED-D91F-EBF8-48F0-1837F9A300FE47830infoc; b_nut=1762936547; i-wanna-go-back=-1; _uuid=8C54395C-CF9D-2179-310103-EBC918CEDD9748783infoc; FEED_LIVE_VERSION=V8'
    KEYWORD_POOL = [
        "ç¾é£Ÿ", "æ—…è¡Œ", "å­¦ä¹ ", "èŒå® ", "æ¸¸æˆ", "ç§‘æŠ€", "å¥èº«", "éŸ³ä¹",
        "èˆè¹ˆ", "ç”µå½±", "åŠ¨æ¼«", "æç¬‘", "æ‰‹å·¥", "æ‘„å½±", "ç©¿æ­", "ç¾å¦†",
        "æ±½è½¦", "èŒåœº", "å¿ƒç†å­¦", "å†å²", "æ³•å¾‹", "è‚²å„¿", "è£…ä¿®", "å›­è‰º"
    ]
    # ===================================================
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Cookie': COOKIE,
    }
    
    os.makedirs('covers', exist_ok=True)
    
    # æ–­ç‚¹ç»­ä¼ 
    data_file = 'video_data_perfect.csv'
    if os.path.exists(data_file):
        existing_df = pd.read_csv(data_file)
        last_index = existing_df['filename'].astype(int).max() if len(existing_df) > 0 else 0
        all_data = existing_df.to_dict('records')
        print(f"ğŸ“‚ åŠ è½½å·²æœ‰æ•°æ®: {len(existing_df)} æ¡")
    else:
        all_data = []
        last_index = 0
        print(f"ğŸ†• æ–°ä»»åŠ¡ï¼Œç›®æ ‡: {target_total} æ¡")
    
    current_index = last_index
    remaining = target_total - current_index
    
    if remaining <= 0:
        print(f"âœ… ç›®æ ‡å·²è¾¾æˆï¼å½“å‰: {current_index} æ¡")
        return pd.DataFrame(all_data)
    
    print(f"ğŸ¯ ç›®æ ‡: {target_total} | è¿˜éœ€: {remaining}")
    
    for keyword in KEYWORD_POOL:
        if current_index >= target_total:
            break
        
        print(f"\n{'='*50}")
        print(f"ğŸ” {keyword} | è¿›åº¦: {current_index}/{target_total}")
        print(f"{'='*50}")
        
        for page in tqdm(range(1, pages_per_keyword + 1), desc=f"{keyword}", ncols=80):
            if current_index >= target_total:
                break
            
            # é¡µé—´å»¶è¿Ÿ
            page_delay = random.uniform(5, 8)
            time.sleep(page_delay)
            
            try:
                url = f"https://api.bilibili.com/x/web-interface/search/type?search_type=video&keyword={urllib.parse.quote(keyword)}&page={page}"
                response = requests.get(url, headers=headers, timeout=20)
                
                if response.status_code == 412:
                    print(f"\nâš ï¸ åçˆ¬ï¼Œæš‚åœ10åˆ†é’Ÿ...")
                    time.sleep(600)
                    continue
                elif response.status_code != 200:
                    print(f"\nâŒ çŠ¶æ€ç : {response.status_code}")
                    time.sleep(10)
                    continue
                
                json_data = response.json()
                
                if json_data.get('code') != 0:
                    print(f"\nâŒ APIé”™è¯¯: {json_data.get('message')}")
                    break
                
                videos = json_data['data'].get('result', [])
                
                success_count = 0
                fail_count = 0
                
                for video in videos:
                    if current_index >= target_total:
                        break
                    
                    try:
                        # ==================== å®Œç¾URLå¤„ç†é€»è¾‘ ====================
                        # 1. å®‰å…¨è·å–
                        pic_url = video.get('pic', '').strip()
                        
                        # 2. ç©ºå€¼æ£€æŸ¥
                        if not pic_url:
                            fail_count += 1
                            continue
                        
                        # 3. ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        if isinstance(pic_url, list):
                            pic_url = pic_url[0] if pic_url else ''
                        elif not isinstance(pic_url, str):
                            pic_url = str(pic_url)
                        
                        # 4. è§„èŒƒåŒ–å¤„ç†ï¼ˆå¤„ç†æ‰€æœ‰å¯èƒ½æ ¼å¼ï¼‰
                        if pic_url.startswith('//'):
                            cover_url = 'https:' + pic_url
                        elif pic_url.startswith('/bfs/'):
                            cover_url = 'https://i0.hdslb.com' + pic_url
                        elif pic_url.startswith('http://'):
                            cover_url = pic_url.replace('http://', 'https://')
                        else:
                            cover_url = pic_url
                        
                        # 5. æœ€ç»ˆéªŒè¯
                        if not cover_url.startswith('https://'):
                            fail_count += 1
                            continue
                        
                        # ==================== ä¸‹è½½ ====================
                        download_delay = random.uniform(1, 2)
                        time.sleep(download_delay)
                        
                        img_response = requests.get(cover_url, timeout=15, headers={'User-Agent': headers['User-Agent']})
                        
                        if img_response.status_code != 200:
                            fail_count += 1
                            print(f"  âŒ ä¸‹è½½å¤±è´¥: {img_response.status_code}")
                            continue
                        
                        if len(img_response.content) < 1024:
                            fail_count += 1
                            continue
                        
                        current_index += 1
                        filename = f"{current_index}.jpg"
                        
                        with open(f'covers/{filename}', 'wb') as f:
                            f.write(img_response.content)
                        
                        # CSVç²¾ç®€
                        all_data.append({
                            'filename': filename,
                            'play_count': int(video.get('play', 0)),
                            'like_count': int(video.get('like', 0))
                        })
                        
                        success_count += 1
                        
                        # è¿›åº¦æ˜¾ç¤º
                        if current_index % 10 == 0:
                            print(f"  âœ… {current_index}/{target_total} ({current_index/target_total*100:.1f}%)")
                        
                    except Exception as e:
                        fail_count += 1
                        # å…³é”®ä¿®å¤ï¼šä¸æ‰“å°å˜é‡å
                        print(f"  âŒ {type(e).__name__}: {str(e)[:30]}")
                        continue
                
                print(f"ğŸ“„ ç¬¬{page}é¡µ: âœ“{success_count} âœ—{fail_count}")
                
            except Exception as e:
                print(f"\nâŒ é¡µè¯·æ±‚å¤±è´¥: {type(e).__name__}")
                time.sleep(10)
                continue
        
        if current_index < target_total:
            batch_delay = random.uniform(300, 400)
            print(f"\nâ˜• ä¼‘æ¯ {batch_delay/60:.1f} åˆ†é’Ÿ...")
            time.sleep(batch_delay)
    
    # æœ€ç»ˆä¿å­˜
    df_final = pd.DataFrame(all_data)
    df_final.to_csv(data_file, index=False)
    
    print(f"\n{'='*50}")
    print(f"ğŸ å®Œæˆï¼æ€»æ•°æ®: {len(df_final)} æ¡")
    print(f"CSVæ ¼å¼: filename,play_count,like_count")
    print(f"å°é¢: covers/1.jpg ~ covers/{current_index}.jpg")
    print(f"{'='*50}")
    
    return df_final

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨Bç«™çˆ¬è™«ï¼ˆå®Œç¾ä¿®å¤ç‰ˆï¼‰")
    print("="*50)
    
    # å…ˆæµ‹è¯•100æ¡
    data = crawl_perfect(
        target_total=10000,
        pages_per_keyword=30
    )