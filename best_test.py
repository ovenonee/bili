# %% [markdown]
# # ğŸ¯ è§†é¢‘å°é¢ â†’ æ’­æ”¾é‡ & ç‚¹èµæ•°ä¸‰åˆ†ç±»ï¼ˆå¹³è¡¡ç‰ˆï¼‰
# âœ… ç»“åˆæœ€ä½³å®è·µï¼Œé¿å…è¿‡æ‹Ÿåˆ

# %%
# ğŸ”§ è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# %%
# ğŸ”§ å¯¼å…¥åº“
import os
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, recall_score
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

# %% [markdown]
# ## ğŸ§  1ï¸âƒ£ æ”¹è¿›çš„Focal Loss

# %%
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        
    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()

# %% [markdown]
# ## ğŸ§  2ï¸âƒ£ ä¼˜åŒ–çš„å¤šä»»åŠ¡æ¨¡å‹ï¼ˆResNet18 + è½»é‡èåˆï¼‰

# %%
class OptimizedMultiTaskModel(nn.Module):
    def __init__(self, num_views=3, num_likes=3, num_labels=32):
        super().__init__()
        
        # ä½¿ç”¨ ResNet18ï¼ˆæ¯”ResNet50æ›´è½»é‡ï¼Œé€‚åˆä½ çš„æ•°æ®ï¼‰
        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # ç§»é™¤æœ€åçš„fcå±‚
        
        # æ ‡ç­¾åµŒå…¥
        self.label_embedding = nn.Embedding(num_labels, 32)
        
        # è½»é‡èåˆåˆ†ç±»å¤´
        fusion_dim = 512 + 32  # ResNet18ç‰¹å¾ + æ ‡ç­¾ç‰¹å¾
        
        self.views_head = nn.Sequential(
            nn.Dropout(0.4),  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(fusion_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_views)
        )
        
        self.likes_head = nn.Sequential(
            nn.Dropout(0.4),  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(fusion_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_likes)
        )
    
    def forward(self, x, label_ids=None):
        # å›¾åƒç‰¹å¾
        img_features = self.backbone(x).view(x.size(0), -1)  # [B, 512]
        
        # æ ‡ç­¾ç‰¹å¾
        if label_ids is not None:
            label_features = self.label_embedding(label_ids)  # [B, 32]
        else:
            label_features = torch.zeros(x.size(0), 32).to(x.device)
        
        # èåˆç‰¹å¾
        fused_features = torch.cat([img_features, label_features], dim=1)
        
        # åˆ†ç±»é¢„æµ‹
        views_pred = self.views_head(fused_features)
        likes_pred = self.likes_head(fused_features)
        
        return views_pred, likes_pred

# %% [markdown]
# ## ğŸ“‚ 3ï¸âƒ£ åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨ä½ çš„è·¯å¾„ï¼‰

# %%
COVER_DIR = r"D:\bilitest\merged_data\covers"
METADATA_PATH = r"D:\bilitest\cleaned_data\result_no0.csv"

if not os.path.exists(METADATA_PATH):
    raise FileNotFoundError(f"âŒ CSV ä¸å­˜åœ¨: {METADATA_PATH}")

df = pd.read_csv(METADATA_PATH)
print(f"ğŸ“Œ åŸå§‹åˆ—å: {df.columns.tolist()}")
print(f"ğŸ“Œ æ€»æ ·æœ¬æ•°: {len(df)}")

df = df.rename(columns={
    'play_count': 'views',
    'like_count': 'likes'
})

required = ['filename', 'views', 'likes', 'label']
missing = [col for col in required if col not in df.columns]
if missing:
    raise ValueError(f"âŒ CSV ç¼ºå°‘åˆ—: {missing}")

# %% [markdown]
# ## ğŸ“Š 4ï¸âƒ£ åˆ†ç±»æ ‡ç­¾ç”Ÿæˆ

# %%
# å¯¹æ•°å˜æ¢
df['log_views'] = np.log10(df['views'] + 1)
df['log_likes'] = np.log10(df['likes'] + 1)

# é‡çº§åˆ†ç®±
VIEWS_THRESH = [0, 5, 7, np.inf]
LIKES_THRESH = [0, 4, 6, np.inf]

df['views_class'] = pd.cut(df['log_views'], bins=VIEWS_THRESH, labels=['ä½', 'ä¸­', 'é«˜'], include_lowest=True)
df['likes_class'] = pd.cut(df['log_likes'], bins=LIKES_THRESH, labels=['ä½', 'ä¸­', 'é«˜'], include_lowest=True)

print("ğŸ“ˆ æ’­æ”¾é‡ç±»åˆ«åˆ†å¸ƒï¼š")
print(df['views_class'].value_counts().sort_index())

# ç¼–ç ç±»åˆ«
le_views = LabelEncoder()
le_likes = LabelEncoder()
df['views_label'] = le_views.fit_transform(df['views_class'])
df['likes_label'] = le_likes.fit_transform(df['likes_class'])

# æ ‡ç­¾ç¼–ç 
le_labels = LabelEncoder()
df['label_encoded'] = le_labels.fit_transform(df['label'])

print(f"ğŸ”¤ æ ‡ç­¾ç§ç±»: {len(le_labels.classes_)} ä¸ª")

# %% [markdown]
# ## ğŸ–¼ï¸ 5ï¸âƒ£ æ•°æ®é¢„å¤„ç†

# %%
df['file_path'] = df['filename'].apply(lambda x: os.path.join(COVER_DIR, str(x)))

print("ğŸ” æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶å­˜åœ¨æ€§...")
df = df[df['file_path'].apply(os.path.exists)]
print(f"âœ… æœ‰æ•ˆå›¾ç‰‡: {len(df)}")

class CoverDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform
        self.valid_indices = []
        print("ğŸ” æ‰«ææœ‰æ•ˆå›¾ç‰‡ä¸­...")
        for i, row in tqdm(self.df.iterrows(), total=len(self.df)):
            try:
                img = Image.open(row['file_path']).convert('RGB')
                if min(img.size) >= 20:
                    self.valid_indices.append(i)
            except Exception as e:
                pass
        print(f"âœ… æœ‰æ•ˆå›¾ç‰‡: {len(self.valid_indices)} / {len(self.df)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        row = self.df.iloc[real_idx]
        img = Image.open(row['file_path']).convert('RGB')
        if self.transform:
            img = self.transform(img)
        
        return {
            'image': img,
            'views_class': row['views_class'],
            'likes_class': row['likes_class'],
            'views_label': row['views_label'],
            'likes_label': row['likes_label'],
            'label_id': row['label_encoded']
        }

# å¼ºåŒ–æ•°æ®å¢å¼ºï¼ˆä½†ä¸è¿‡åº¦ï¼‰
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),  # å‡è½»å¢å¼ºå¼ºåº¦
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# åˆ’åˆ†æ•°æ®é›†
train_df, val_df = train_test_split(
    df,
    test_size=0.2,
    stratify=df[['views_class', 'likes_class']].apply(tuple, axis=1),
    random_state=42
)

train_dataset = CoverDataset(train_df, transform=train_transform)
val_dataset = CoverDataset(val_df, transform=val_transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)  # å¢å¤§batch_size
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)

print(f"ğŸ§® è®­ç»ƒé›†: {len(train_dataset)} | éªŒè¯é›†: {len(val_dataset)}")

# %% [markdown]
# ## ğŸ§  6ï¸âƒ£ æ¨¡å‹åˆå§‹åŒ–

# %%
model = OptimizedMultiTaskModel(
    num_views=len(le_views.classes_),
    num_likes=len(le_likes.classes_),
    num_labels=len(le_labels.classes_)
).to(device)

print(f"âœ… æ¨¡å‹: OptimizedMultiTaskModel (ResNet18 + Label Embedding)")

# %% [markdown]
# ## âš™ï¸ 7ï¸âƒ£ è®­ç»ƒè®¾ç½®

# %%
# ä½¿ç”¨ Focal Loss
criterion_views = FocalLoss(alpha=1, gamma=2).to(device)
criterion_likes = FocalLoss(alpha=1, gamma=2).to(device)

# ä¼˜åŒ–å™¨ï¼šä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡é˜²æ­¢è¿‡æ‹Ÿåˆ
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)

EPOCHS = 15  # å‡å°‘è®­ç»ƒè½®æ•°é˜²æ­¢è¿‡æ‹Ÿåˆ

# %% [markdown]
# ## ğŸ‹ï¸ 8ï¸âƒ£ è®­ç»ƒå‡½æ•°

# %%
def train_one_epoch(model, loader, crit_v, crit_l, optimizer, device):
    model.train()
    total_loss = 0.0
    
    for batch in tqdm(loader, desc="è®­ç»ƒä¸­"):
        images = batch['image'].to(device, non_blocking=True)
        label_ids = batch['label_id'].to(device, non_blocking=True)
        v_labels = batch['views_label'].to(device, non_blocking=True)
        l_labels = batch['likes_label'].to(device, non_blocking=True)

        optimizer.zero_grad()
        
        v_out, l_out = model(images, label_ids)
        loss_v = crit_v(v_out, v_labels)
        loss_l = crit_l(l_out, l_labels)
        loss = loss_v + loss_l

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    return total_loss / len(loader)

def evaluate(model, loader, device, le_v, le_l):
    model.eval()
    v_true, v_pred = [], []
    l_true, l_pred = [], []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="éªŒè¯ä¸­"):
            images = batch['image'].to(device, non_blocking=True)
            label_ids = batch['label_id'].to(device, non_blocking=True)
            v_labels = batch['views_label'].numpy()
            l_labels = batch['likes_label'].numpy()

            v_out, l_out = model(images, label_ids)
            _, v_p = torch.max(v_out, 1)
            _, l_p = torch.max(l_out, 1)

            v_pred.extend(v_p.cpu().numpy())
            l_pred.extend(l_p.cpu().numpy())
            v_true.extend(v_labels)
            l_true.extend(l_labels)
    
    acc_v = accuracy_score(v_true, v_pred)
    acc_l = accuracy_score(l_true, l_pred)
    return acc_v, acc_l, v_true, v_pred, l_true, l_pred

# %% [markdown]
# ## â–¶ï¸ 9ï¸âƒ£ å¼€å§‹è®­ç»ƒ

# %%
print("\nğŸ”¥ å¼€å§‹ä¼˜åŒ–è®­ç»ƒï¼ˆResNet18 + Focal Loss + Label Embeddingï¼‰...")
best_avg_acc = 0.0
patience_counter = 0
max_patience = 5

for epoch in range(EPOCHS):
    loss = train_one_epoch(model, train_loader, criterion_views, criterion_likes, optimizer, device)
    acc_v, acc_l, _, _, _, _ = evaluate(model, val_loader, device, le_views, le_likes)
    avg_acc = (acc_v + acc_l) / 2

    print(f"Epoch {epoch+1:2d}/{EPOCHS} | Loss: {loss:.4f} | "
          f"Views Acc: {acc_v:.2%} | Likes Acc: {acc_l:.2%} | Avg: {avg_acc:.2%}")

    if avg_acc > best_avg_acc:
        best_avg_acc = avg_acc
        torch.save(model.state_dict(), r"D:\bilitest\modual\best_optimized_model.pth")
        print("   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= max_patience:
            print(f"   ğŸ›‘ æ—©åœè§¦å‘ï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_avg_acc:.2%}")
            break

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(avg_acc)

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load(r"D:\bilitest\modual\best_optimized_model.pth", map_location=device))
print(f"\nğŸ† æœ€ä½³éªŒè¯å¹³å‡å‡†ç¡®ç‡: {best_avg_acc:.2%}")

# %% [markdown]
# ## ğŸ“ˆ ğŸ“Š 10ï¸âƒ£ æœ€ç»ˆè¯„ä¼°

# %%
acc_v, acc_l, v_true, v_pred, l_true, l_pred = evaluate(model, val_loader, device, le_views, le_likes)

print(f"\nâœ… æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡ â†’ æ’­æ”¾é‡: {acc_v:.2%} | ç‚¹èµæ•°: {acc_l:.2%}")

# é«˜ç±»å¬å›ç‡
high_idx_v = list(le_views.classes_).index('é«˜')
high_idx_l = list(le_likes.classes_).index('é«˜')

try:
    high_recall_v = recall_score(v_true, v_pred, labels=[high_idx_v], average=None)[0]
    high_recall_l = recall_score(l_true, l_pred, labels=[high_idx_l], average=None)[0]
    print(f"\nğŸ¯ é«˜æ’­æ”¾é‡å¬å›ç‡: {high_recall_v:.2%} | é«˜ç‚¹èµå¬å›ç‡: {high_recall_l:.2%}")
except:
    print(f"\nâš ï¸ é«˜ç±»å¬å›ç‡è®¡ç®—å¤±è´¥")

print("\nğŸ“Š æ’­æ”¾é‡åˆ†ç±»æŠ¥å‘Šï¼š")
print(classification_report(v_true, v_pred, target_names=le_views.classes_, digits=3))

print("\nğŸ“Š ç‚¹èµæ•°åˆ†ç±»æŠ¥å‘Šï¼š")
print(classification_report(l_true, l_pred, target_names=le_likes.classes_, digits=3))

print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_avg_acc:.2%}")
print("æ¨¡å‹å·²ä¿å­˜ä¸º: D:\bilitest\modual\best_optimized_model2.pth")