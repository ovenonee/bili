# bili_massive_crawler.py
import requests
import time
import os
import pandas as pd
import urllib.parse
from tqdm import tqdm

def crawl_massive_data(target_total=10000, keywords_per_batch=5, pages_per_keyword=50):
    """
    å¤§è§„æ¨¡Bç«™çˆ¬è™«ï¼šè‡ªåŠ¨å¾ªç¯å…³é”®è¯ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œç›®æ ‡10,000æ¡æ•°æ®
    
    å‚æ•°:
        target_total: æ€»ç›®æ ‡æ•°æ®é‡ï¼ˆé»˜è®¤10000ï¼‰
        keywords_per_batch: æ¯æ‰¹åŒæ—¶çˆ¬å–çš„å…³é”®è¯æ•°é‡ï¼ˆé»˜è®¤5ä¸ªï¼‰
        pages_per_keyword: æ¯ä¸ªå…³é”®è¯çˆ¬å–çš„é¡µæ•°ï¼ˆæ¯é¡µ20æ¡ï¼Œé»˜è®¤50é¡µ=1000æ¡ï¼‰
    """
    
    # ==================== æ ¸å¿ƒé…ç½®åŒºï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰ ====================
    COOKIE = 'buvid3=B1EDF9ED-D91F-EBF8-48F0-1837F9A300FE47830infoc; b_nut=1762936547; i-wanna-go-back=-1; _uuid=8C54395C-CF9D-2179-310103-EBC918CEDD9748783infoc; FEED_LIVE_VERSION=V8'
    
    # å¯æ‰©å±•å…³é”®è¯æ± ï¼ˆå»ºè®®å‡†å¤‡20+ä¸ªå…³é”®è¯å¾ªç¯ä½¿ç”¨ï¼‰
    KEYWORD_POOL = [
        "ç¾é£Ÿ", "æ—…è¡Œ", "å­¦ä¹ ", "èŒå® ", "æ¸¸æˆ", "ç§‘æŠ€", "å¥èº«", "éŸ³ä¹", 
        "èˆè¹ˆ", "ç”µå½±", "åŠ¨æ¼«", "æç¬‘", "æ‰‹å·¥", "æ‘„å½±", "ç©¿æ­", "ç¾å¦†",
        "æ±½è½¦", "èŒåœº", "å¿ƒç†å­¦", "å†å²", "æ³•å¾‹", "è‚²å„¿", "è£…ä¿®", "å›­è‰º"
    ]
    # ============================================================
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cookie': COOKIE,
    }
    
    os.makedirs('covers', exist_ok=True)
    
    # æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²æœ‰æ•°æ®
    data_file = 'video_data_massive.csv'
    if os.path.exists(data_file):
        existing_df = pd.read_csv(data_file)
        seen_urls = set(existing_df['cover_url'].tolist())
        all_data = existing_df.to_dict('records')
        start_count = len(existing_df)
        print(f"ğŸ“‚ åŠ è½½å·²æœ‰æ•°æ®: {start_count} æ¡")
    else:
        all_data = []
        seen_urls = set()
        start_count = 0
    
    # è®¡ç®—éœ€è¦çˆ¬å–çš„é‡
    remaining = target_total - start_count
    if remaining <= 0:
        print(f"âœ… ç›®æ ‡å·²è¾¾æˆï¼å½“å‰æ•°æ®é‡: {start_count}")
        return pd.DataFrame(all_data)
    
    print(f"ğŸ¯ ç›®æ ‡æ€»é‡: {target_total} | è¿˜éœ€çˆ¬å–: {remaining}")
    
    # è‡ªåŠ¨è®¡ç®—éœ€è¦çš„å…³é”®è¯æ‰¹æ¬¡
    needed_batches = -(-remaining // (keywords_per_batch * pages_per_keyword * 20))  # å‘ä¸Šå–æ•´
    print(f"é¢„è®¡éœ€è¦ {needed_batches} ä¸ªå…³é”®è¯æ‰¹æ¬¡")
    
    batch_count = 0
    
    # ä¸»å¾ªç¯ï¼šæŒ‰æ‰¹æ¬¡å¤„ç†å…³é”®è¯
    for i in range(0, len(KEYWORD_POOL), keywords_per_batch):
        if len(all_data) >= target_total:
            break
            
        current_keywords = KEYWORD_POOL[i:i+keywords_per_batch]
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_count+1}/{needed_batches} | å…³é”®è¯: {current_keywords}")
        print(f"{'='*60}")
        
        # éå†å½“å‰æ‰¹æ¬¡çš„å…³é”®è¯
        for keyword in current_keywords:
            if len(all_data) >= target_total:
                print("ğŸ‰ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œæå‰ç»“æŸ")
                break
            
            print(f"\nğŸ” æ­£åœ¨çˆ¬å–: '{keyword}'")
            
            # ä¸ºæ¯ä¸ªå…³é”®è¯åˆ›å»ºç‹¬ç«‹è¿›åº¦æ¡
            for page in tqdm(range(1, pages_per_keyword + 1), 
                           desc=f"{keyword:>6s}", 
                           ncols=80):
                
                success = False
                retry_count = 0
                
                while not success and retry_count < 3:
                    try:
                        # å…³é”®ä¿®å¤ï¼šURLç¼–ç æ­£ç¡®ï¼Œæ— ç©ºæ ¼
                        encoded_keyword = urllib.parse.quote(keyword)
                        url = f"https://api.bilibili.com/x/web-interface/search/type?search_type=video&keyword={encoded_keyword}&page={page}"
                        
                        response = requests.get(url, headers=headers, timeout=15)
                        
                        # åçˆ¬å¤„ç†
                        if response.status_code == 412:
                            print(f"\nâš ï¸ è§¦å‘åçˆ¬ï¼Œæš‚åœ60ç§’...")
                            time.sleep(60)
                            retry_count += 1
                            continue
                        elif response.status_code != 200:
                            print(f"\nâŒ çŠ¶æ€ç å¼‚å¸¸: {response.status_code}")
                            retry_count += 1
                            time.sleep(10)
                            continue
                        
                        json_data = response.json()
                        
                        if json_data.get('code') != 0:
                            print(f"\nâŒ APIé”™è¯¯: {json_data.get('message')}")
                            break
                        
                        videos = json_data['data'].get('result', [])
                        
                        for video in videos:
                            if len(all_data) >= target_total:
                                success = True
                                break
                            
                            try:
                                cover_url = video.get('pic', '')
                                if cover_url.startswith('//'):
                                    cover_url = 'https:' + cover_url
                                
                                if cover_url in seen_urls or not cover_url:
                                    continue
                                
                                title = video.get('title', '').replace('<em class="keyword">', '').replace('</em>', '')
                                
                                all_data.append({
                                    'keyword': keyword,
                                    'title': title,
                                    'cover_url': cover_url,
                                    'play_count': str(video.get('play', 0)),
                                    'like_count': str(video.get('like', 0))
                                })
                                
                                seen_urls.add(cover_url)
                                
                                # æ¯50æ¡ä¿å­˜ä¸€æ¬¡
                                if len(all_data) % 50 == 0:
                                    df_temp = pd.DataFrame(all_data)
                                    df_temp.to_csv(data_file, index=False, encoding='utf-8-sig')
                                    print(f"\nğŸ’¾ è‡ªåŠ¨ä¿å­˜: {len(all_data)} æ¡")
                                
                                # ä¸‹è½½å°é¢
                                img_response = requests.get(cover_url, timeout=10, headers={'User-Agent': headers['User-Agent']})
                                with open(f'covers/{len(all_data)}.jpg', 'wb') as f:
                                    f.write(img_response.content)
                                
                                time.sleep(0.3)  # è¯·æ±‚é—´éš”
                                
                            except Exception as e:
                                print(f"  è§£æå¤±è´¥: {e}")
                                continue
                        
                        success = True
                        time.sleep(1.5)  # é¡µé¢é—´éš”
                        
                    except Exception as e:
                        print(f"\n  è¯·æ±‚å¤±è´¥: {e}ï¼Œé‡è¯• {retry_count+1}/3")
                        retry_count += 1
                        time.sleep(5)
        
        batch_count += 1
        
        # æ‰¹æ¬¡é—´ä¼‘æ¯
        if len(all_data) < target_total:
            print(f"\nâ˜• æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯30ç§’...")
            time.sleep(30)
    
    # æœ€ç»ˆä¿å­˜
    df_final = pd.DataFrame(all_data)
    df_final.to_csv(data_file, index=False, encoding='utf-8-sig')
    
    # ç»Ÿè®¡æŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“Š æ€»è®¡æ•°æ®: {len(df_final)} æ¡")
    print(f"ğŸ¯ ç›®æ ‡å®Œæˆåº¦: {len(df_final)}/{target_total} ({len(df_final)/target_total*100:.1f}%)")
    print(f"ğŸ“ˆ å…³é”®è¯åˆ†å¸ƒ:\n{df_final['keyword'].value_counts().head(10)}")
    print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜: {data_file}")
    print(f"{'='*60}")
    
    return df_final

if __name__ == "__main__":
    # ==================== å‚æ•°é…ç½® ====================
    # ç›®æ ‡ï¼šçˆ¬å–10,000æ¡æ•°æ®
    # ç­–ç•¥ï¼š4ä¸ªå…³é”®è¯ Ã— 125é¡µ = 10,000æ¡ï¼ˆæ¯é¡µ20æ¡ï¼‰
    
    print("ğŸš€ å¯åŠ¨å¤§è§„æ¨¡Bç«™çˆ¬è™«")
    print("="*60)
    
    # å¿«é€Ÿå¼€å§‹ï¼šç›´æ¥è¿è¡Œé»˜è®¤é…ç½®
    data = crawl_massive_data(
        target_total=10000,
        keywords_per_batch=4,      # æ¯æ‰¹4ä¸ªå…³é”®è¯
        pages_per_keyword=125      # æ¯ä¸ªå…³é”®è¯125é¡µ
    )
    
    # å¦‚éœ€è‡ªå®šä¹‰ï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š
    # data = crawl_massive_data(
    #     target_total=5000,       # ç›®æ ‡5000æ¡
    #     keywords_per_batch=2,    # æ¯æ‰¹2ä¸ªå…³é”®è¯
    #     pages_per_keyword=50     # æ¯ä¸ªå…³é”®è¯50é¡µ
    # )