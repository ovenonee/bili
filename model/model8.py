import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ========================
# 1. åŠ è½½æ•°æ®
# ========================
df = pd.read_csv(r'D:\bilitest\cleaned_data\result_no0.csv')  # è¯·æ›¿æ¢ä¸ºä½ çš„ CSV æ–‡ä»¶å
# å‡è®¾åˆ—åæ˜¯: filename, play_count, like_count, label
print("åŸå§‹æ•°æ®å½¢çŠ¶:", df.shape)
print(df.head())

# åŠ è½½å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
visual_features = np.load(r'D:\bilitest\features\visual_X.npy')  # å½¢çŠ¶: (N, D1)
text_features = np.load(r'D:\bilitest\features\text_X.npy')      # å½¢çŠ¶: (N, D2)

assert len(df) == len(visual_features) == len(text_features), "æ ·æœ¬æ•°ä¸ä¸€è‡´ï¼"

# åˆå¹¶ç‰¹å¾
X = np.concatenate([visual_features, text_features], axis=1)

# ========================
# 2. åˆ›å»ºåˆ†ç±»æ ‡ç­¾ï¼ˆä»…ä½¿ç”¨åŸå§‹æ’­æ”¾é‡è¿›è¡Œåˆ†ç±»ï¼Œä¸ä½œä¸ºç‰¹å¾ï¼‰
# ========================
play_counts = df['play_count'].values
q25 = np.percentile(play_counts, 25)
q75 = np.percentile(play_counts, 75)

def classify_hotness(pc):
    if pc < q25:
        return 'ä½çƒ­åº¦'
    elif pc < q75:
        return 'ä¸­çƒ­åº¦'
    else:
        return 'é«˜çƒ­åº¦'

hotness_labels = np.array([classify_hotness(pc) for pc in play_counts])
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(hotness_labels)

print(f"çƒ­åº¦åˆ’åˆ†é˜ˆå€¼: ä½(<{q25:.0f}), ä¸­({q25:.0f}~{q75:.0f}), é«˜(>{q75:.0f})")
print(f"çƒ­åº¦åˆ†å¸ƒ: {np.unique(hotness_labels, return_counts=True)}")

# ========================
# 3. ç‰¹å¾å·¥ç¨‹ï¼ˆä»…ä½¿ç”¨ä¸ä¼šæ³„éœ²ç›®æ ‡ä¿¡æ¯çš„ç‰¹å¾ï¼‰
# ========================
like_counts = df['like_count'].values

# 1. ç‚¹èµ/æ’­æ”¾æ¯”ç‡ï¼ˆå¯¹æ•°ï¼‰- è¿™ä¸ªæ˜¯å¯ä»¥çš„ï¼Œå› ä¸ºå®é™…åº”ç”¨ä¸­å¯èƒ½æœ‰æ ‡é¢˜ä¿¡æ¯
ratio_log = np.log1p(like_counts / (play_counts + 1e-8))

# åˆå¹¶ç‰¹å¾ï¼ˆä»…ä½¿ç”¨å›¾åƒ+æ–‡æœ¬ç‰¹å¾ + å®‰å…¨çš„æ¯”ç‡ç‰¹å¾ï¼‰
X_enhanced = np.column_stack([
    X,                    # åŸå§‹å›¾åƒ+æ–‡æœ¬ç‰¹å¾ (367ç»´)
    ratio_log             # ç‚¹èµ/æ’­æ”¾æ¯” (1ç»´) - è¿™æ˜¯å”¯ä¸€å¯ä»¥æ·»åŠ çš„é¢å¤–ç‰¹å¾
])

print(f"å¢å¼ºåç‰¹å¾ç»´åº¦: {X_enhanced.shape[1]}")

# ========================
# 4. æ•°æ®é¢„å¤„ç†
# ========================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_enhanced)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y, shuffle=True
)

# ========================
# 5. è®­ç»ƒæ¨¡å‹
# ========================
try:
    from xgboost import XGBClassifier
    
    print("ğŸš€ ä½¿ç”¨ä¼˜åŒ–çš„ XGBoost æ¨¡å‹...")
    
    classifier = XGBClassifier(
        n_estimators=300,
        max_depth=10,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.1,
        random_state=42,
        objective='multi:softprob',
        eval_metric='mlogloss',
        n_jobs=1
    )
    
    print("å¼€å§‹è®­ç»ƒ XGBoost åˆ†ç±»å™¨...")
    classifier.fit(X_train, y_train)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    
    model_name = 'XGBoost_Corrected'
    
except ImportError:
    print("âŒ æœªå®‰è£… xgboostï¼Œä½¿ç”¨éšæœºæ£®æ—")
    from sklearn.ensemble import RandomForestClassifier
    
    classifier = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        random_state=42,
        n_jobs=1
    )
    
    print("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨...")
    classifier.fit(X_train, y_train)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    
    model_name = 'RandomForest_Corrected'

# ========================
# 6. è¯„ä¼°æ¨¡å‹
# ========================
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')

print(f"\n=== æ¨¡å‹è¯„ä¼°ç»“æœ ({model_name}) ===")
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"F1å®å¹³å‡: {f1_macro:.4f}")

target_names = label_encoder.classes_
print("\n=== è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ===")
print(classification_report(y_test, y_pred, target_names=target_names))

cm = confusion_matrix(y_test, y_pred)
print("\n=== æ··æ·†çŸ©é˜µ ===")
print(cm)

# ========================
# 7. å¯è§†åŒ–ç»“æœ
# ========================
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.title(f'{model_name} æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.tight_layout()
plt.show()

# ========================
# 8. ä¿å­˜æ¨¡å‹
# ========================
joblib.dump(classifier, f'bilibili_classifier_{model_name.lower()}.pkl')
joblib.dump(label_encoder, 'label_encoder.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜ä¸º 'bilibili_classifier_{model_name.lower()}.pkl'")

# ========================
# 9. ç¤ºä¾‹é¢„æµ‹
# ========================
print("\n=== ç¤ºä¾‹é¢„æµ‹ ===")
y_pred_proba = classifier.predict_proba(X_test)
sample_idx = np.random.choice(len(X_test), 5, replace=False)

for i, idx in enumerate(sample_idx):
    true_label = label_encoder.inverse_transform([y_test[idx]])[0]
    pred_label = label_encoder.inverse_transform([y_pred[idx]])[0]
    prob = y_pred_proba[idx]
    print(f"æ ·æœ¬ {i+1}: çœŸå®={true_label}, é¢„æµ‹={pred_label}, "
          f"æ¦‚ç‡=[ä½:{prob[0]:.2f}, ä¸­:{prob[1]:.2f}, é«˜:{prob[2]:.2f}]")

# ========================
# 10. ç‰¹å¾é‡è¦æ€§åˆ†æ
# ========================
try:
    feature_importance = classifier.feature_importances_
    top_features_idx = np.argsort(feature_importance)[-10:]
    top_importance = feature_importance[top_features_idx]
    
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(top_importance)), top_importance)
    plt.yticks(range(len(top_importance)), [f'Feature_{i}' for i in top_features_idx])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'{model_name} å‰10ä¸ªé‡è¦ç‰¹å¾')
    plt.tight_layout()
    plt.show()
    
    print(f"\n=== å‰5ä¸ªé‡è¦ç‰¹å¾ç´¢å¼• ===")
    for i in range(5):
        idx = top_features_idx[-(i+1)]
        imp = top_importance[-(i+1)]
        print(f"ç‰¹å¾ {idx}: é‡è¦æ€§ = {imp:.4f}")
        
except AttributeError:
    print("å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")

# ========================
# 11. äº¤å‰éªŒè¯è¯„ä¼°
# ========================
print("\nğŸ” è¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°...")
cv_scores = cross_val_score(
    classifier, 
    X_scaled, y, 
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1_macro',
    n_jobs=1
)

print(f"5æŠ˜äº¤å‰éªŒè¯F1å®å¹³å‡åˆ†æ•°: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")