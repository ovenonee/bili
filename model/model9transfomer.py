import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# æ£€æŸ¥æ˜¯å¦æœ‰GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ========================
# 1. è‡ªå®šä¹‰æ•°æ®é›†ç±»
# ========================
class BiliDataset(Dataset):
    def __init__(self, visual_features, text_features, labels):
        self.visual_features = torch.FloatTensor(visual_features)
        self.text_features = torch.FloatTensor(text_features)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return self.visual_features[idx], self.text_features[idx], self.labels[idx]

# ========================
# 2. å¤šæ¨¡æ€Transformeræ¨¡å‹
# ========================
class MultiModalTransformer(nn.Module):
    def __init__(self, visual_dim, text_dim, num_classes, d_model=256, nhead=8, num_layers=2):
        super(MultiModalTransformer, self).__init__()
        
        # ç‰¹å¾æŠ•å½±å±‚
        self.visual_proj = nn.Linear(visual_dim, d_model)
        self.text_proj = nn.Linear(text_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(2, d_model))  # 2ä¸ªæ¨¡æ€
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model*2,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(d_model//2, num_classes)
        )
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, visual_features, text_features):
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        visual_proj = self.visual_proj(visual_features).unsqueeze(1)  # (batch, 1, d_model)
        text_proj = self.text_proj(text_features).unsqueeze(1)      # (batch, 1, d_model)
        
        # æ‹¼æ¥ä¸¤ä¸ªæ¨¡æ€
        combined = torch.cat([visual_proj, text_proj], dim=1)       # (batch, 2, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        combined = combined + self.pos_encoding.unsqueeze(0)        # (batch, 2, d_model)
        
        # Transformerå¤„ç†
        output = self.transformer(combined)                         # (batch, 2, d_model)
        
        # å–å¹³å‡æˆ–æ‹¼æ¥ï¼ˆè¿™é‡Œå–å¹³å‡ï¼‰
        fused_features = output.mean(dim=1)                         # (batch, d_model)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)                    # (batch, num_classes)
        
        return logits

# ========================
# 3. åŠ è½½æ•°æ®
# ========================
df = pd.read_csv(r'D:\bilitest\cleaned_data\result_no0.csv')  # è¯·æ›¿æ¢ä¸ºä½ çš„ CSV æ–‡ä»¶å
# å‡è®¾åˆ—åæ˜¯: filename, play_count, like_count, label
print("åŸå§‹æ•°æ®å½¢çŠ¶:", df.shape)
print(df.head())

# åŠ è½½å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
visual_features = np.load(r'D:\bilitest\features\visual_X.npy')  # å½¢çŠ¶: (N, D1)
text_features = np.load(r'D:\bilitest\features\text_X.npy')      # å½¢çŠ¶: (N, D2)

assert len(df) == len(visual_features) == len(text_features), "æ ·æœ¬æ•°ä¸ä¸€è‡´ï¼"

# åˆ›å»ºåˆ†ç±»æ ‡ç­¾
play_counts = df['play_count'].values
q25 = np.percentile(play_counts, 25)
q75 = np.percentile(play_counts, 75)

def classify_hotness(pc):
    if pc < q25:
        return 'ä½çƒ­åº¦'
    elif pc < q75:
        return 'ä¸­çƒ­åº¦'
    else:
        return 'é«˜çƒ­åº¦'

hotness_labels = np.array([classify_hotness(pc) for pc in play_counts])
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(hotness_labels)

print(f"çƒ­åº¦åˆ’åˆ†é˜ˆå€¼: ä½(<{q25:.0f}), ä¸­({q25:.0f}~{q75:.0f}), é«˜(>{q75:.0f})")
print(f"çƒ­åº¦åˆ†å¸ƒ: {np.unique(hotness_labels, return_counts=True)}")

# ========================
# 4. æ•°æ®é¢„å¤„ç†
# ========================
# æ ‡å‡†åŒ–ç‰¹å¾
scaler_visual = StandardScaler()
scaler_text = StandardScaler()

visual_scaled = scaler_visual.fit_transform(visual_features)
text_scaled = scaler_text.fit_transform(text_features)

# åˆ’åˆ†æ•°æ®
X_visual_train, X_visual_test, X_text_train, X_text_test, y_train, y_test = train_test_split(
    visual_scaled, text_scaled, y, test_size=0.2, random_state=42, stratify=y, shuffle=True
)

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
train_dataset = BiliDataset(X_visual_train, X_text_train, y_train)
test_dataset = BiliDataset(X_visual_test, X_text_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}, æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
print(f"è§†è§‰ç‰¹å¾ç»´åº¦: {visual_scaled.shape[1]}, æ–‡æœ¬ç‰¹å¾ç»´åº¦: {text_scaled.shape[1]}")

# ========================
# 5. åˆå§‹åŒ–æ¨¡å‹
# ========================
model = MultiModalTransformer(
    visual_dim=visual_scaled.shape[1],
    text_dim=text_scaled.shape[1],
    num_classes=len(np.unique(y)),
    d_model=256,
    nhead=8,
    num_layers=2
).to(device)

print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)

# ========================
# 6. è®­ç»ƒå‡½æ•°
# ========================
def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for visual_batch, text_batch, labels in dataloader:
        visual_batch = visual_batch.to(device)
        text_batch = text_batch.to(device)
        labels = labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(visual_batch, text_batch)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    return total_loss / len(dataloader), 100. * correct / total

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for visual_batch, text_batch, labels in dataloader:
            visual_batch = visual_batch.to(device)
            text_batch = text_batch.to(device)
            labels = labels.to(device)
            
            outputs = model(visual_batch, text_batch)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    return total_loss / len(dataloader), 100. * correct / total, all_preds, all_labels

# ========================
# 7. è®­ç»ƒæ¨¡å‹
# ========================
num_epochs = 50
train_losses = []
train_accuracies = []
val_losses = []
val_accuracies = []

print("ğŸš€ å¼€å§‹è®­ç»ƒå¤šæ¨¡æ€Transformer...")
best_val_acc = 0
patience = 10
patience_counter = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion, device)
    
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)
    
    scheduler.step()
    
    print(f'Epoch [{epoch+1}/{num_epochs}] - '
          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
    
    # æ—©åœ
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save({
            'model_state_dict': model.state_dict(),
            'scaler_visual': scaler_visual,
            'scaler_text': scaler_text,
            'label_encoder': label_encoder
        }, 'multimodal_transformer_best.pth')
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

# ========================
# 8. æœ€ç»ˆè¯„ä¼°
# ========================
# åŠ è½½æœ€ä½³æ¨¡å‹
checkpoint = torch.load('multimodal_transformer_best.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
final_val_loss, final_val_acc, all_preds, all_labels = evaluate(model, test_loader, criterion, device)

print(f"\n=== æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœ ===")
print(f"å‡†ç¡®ç‡: {final_val_acc:.4f}")
print(f"æŸå¤±: {final_val_loss:.4f}")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
target_names = label_encoder.classes_
print("\n=== è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ===")
print(classification_report(all_labels, all_preds, target_names=target_names))

cm = confusion_matrix(all_labels, all_preds)
print("\n=== æ··æ·†çŸ©é˜µ ===")
print(cm)

# ========================
# 9. å¯è§†åŒ–ç»“æœ
# ========================
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®­ç»ƒæ›²çº¿
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', marker='o')
ax1.plot(val_losses, label='éªŒè¯æŸå¤±', marker='s')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('æŸå¤±')
ax1.set_title('è®­ç»ƒè¿‡ç¨‹ - æŸå¤±æ›²çº¿')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', marker='o')
ax2.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', marker='s')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
ax2.set_title('è®­ç»ƒè¿‡ç¨‹ - å‡†ç¡®ç‡æ›²çº¿')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.title('å¤šæ¨¡æ€Transformer - æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.tight_layout()
plt.show()

# ========================
# 10. ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
# ========================
torch.save({
    'model_state_dict': model.state_dict(),
    'scaler_visual': scaler_visual,
    'scaler_text': scaler_text,
    'label_encoder': label_encoder,
    'model_config': {
        'visual_dim': visual_scaled.shape[1],
        'text_dim': text_scaled.shape[1],
        'num_classes': len(np.unique(y)),
        'd_model': 256,
        'nhead': 8,
        'num_layers': 2
    }
}, 'multimodal_transformer_final.pth')

print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜ä¸º 'multimodal_transformer_final.pth'")

# ========================
# 11. ç¤ºä¾‹é¢„æµ‹
# ========================
print("\n=== ç¤ºä¾‹é¢„æµ‹ ===")
model.eval()
with torch.no_grad():
    for i in range(5):
        idx = np.random.randint(0, len(test_dataset))
        visual_feat, text_feat, true_label = test_dataset[idx]
        
        visual_feat = visual_feat.unsqueeze(0).to(device)
        text_feat = text_feat.unsqueeze(0).to(device)
        
        output = model(visual_feat, text_feat)
        pred_label = output.argmax(1).item()
        
        true_label_name = label_encoder.inverse_transform([true_label.item()])[0]
        pred_label_name = label_encoder.inverse_transform([pred_label])[0]
        
        probs = torch.softmax(output, dim=1).cpu().numpy()[0]
        
        print(f"æ ·æœ¬ {i+1}: çœŸå®={true_label_name}, é¢„æµ‹={pred_label_name}, "
              f"æ¦‚ç‡=[ä½:{probs[0]:.2f}, ä¸­:{probs[1]:.2f}, é«˜:{probs[2]:.2f}]")

# ========================
# 12. äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ========================
print(f"\n=== æ¨¡å‹æ€§èƒ½æ€»ç»“ ===")
f1_macro = f1_score(all_labels, all_preds, average='macro')
print(f"å‡†ç¡®ç‡: {final_val_acc:.4f}")
print(f"F1å®å¹³å‡: {f1_macro:.4f}")