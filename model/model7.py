import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ========================
# 1. åŠ è½½æ•°æ®
# ========================
df = pd.read_csv(r'D:\bilitest\cleaned_data\result_no0.csv')  # è¯·æ›¿æ¢ä¸ºä½ çš„ CSV æ–‡ä»¶å
# å‡è®¾åˆ—åæ˜¯: filename, play_count, like_count, label
print("åŸå§‹æ•°æ®å½¢çŠ¶:", df.shape)
print(df.head())

# åŠ è½½å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
visual_features = np.load(r'D:\bilitest\features\visual_X.npy')  # å½¢çŠ¶: (N, D1)
text_features = np.load(r'D:\bilitest\features\text_X.npy')      # å½¢çŠ¶: (N, D2)

assert len(df) == len(visual_features) == len(text_features), "æ ·æœ¬æ•°ä¸ä¸€è‡´ï¼"

# åˆå¹¶ç‰¹å¾
X = np.concatenate([visual_features, text_features], axis=1)

# ã€ç‰¹å¾å·¥ç¨‹ã€‘æ·»åŠ æ›´å¤šç‰¹å¾
like_counts = df['like_count'].values
play_counts = df['play_count'].values

# 1. ç‚¹èµ/æ’­æ”¾æ¯”ç‡ï¼ˆå¯¹æ•°ï¼‰
ratio_log = np.log1p(like_counts / (play_counts + 1e-8))

# 2. æ’­æ”¾é‡çš„å¯¹æ•°
play_log = np.log1p(play_counts)

# 3. ç‚¹èµé‡çš„å¯¹æ•°
like_log = np.log1p(like_counts)

# 4. æ’­æ”¾é‡ä¸ç‚¹èµé‡çš„å·®å€¼ï¼ˆå¯¹æ•°ï¼‰
diff_log = play_log - like_log

# 5. æ’­æ”¾é‡çš„æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
play_zscore = (play_counts - play_counts.mean()) / play_counts.std()

# åˆå¹¶æ‰€æœ‰ç‰¹å¾
X_enhanced = np.column_stack([
    X,                    # åŸå§‹å›¾åƒ+æ–‡æœ¬ç‰¹å¾
    ratio_log,            # ç‚¹èµ/æ’­æ”¾æ¯”
    play_log,             # æ’­æ”¾é‡å¯¹æ•°
    like_log,             # ç‚¹èµé‡å¯¹æ•°
    diff_log,             # å·®å€¼
    play_zscore           # æ’­æ”¾é‡Z-score
])

print(f"å¢å¼ºåç‰¹å¾ç»´åº¦: {X_enhanced.shape[1]} (ä» {X.shape[1]} å¢åŠ åˆ° {X_enhanced.shape[1]})")

# ========================
# 2. åˆ›å»ºåˆ†ç±»æ ‡ç­¾
# ========================
q25 = np.percentile(play_counts, 25)
q75 = np.percentile(play_counts, 75)

def classify_hotness(pc):
    if pc < q25:
        return 'ä½çƒ­åº¦'
    elif pc < q75:
        return 'ä¸­çƒ­åº¦'
    else:
        return 'é«˜çƒ­åº¦'

hotness_labels = np.array([classify_hotness(pc) for pc in play_counts])
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(hotness_labels)

print(f"çƒ­åº¦åˆ’åˆ†é˜ˆå€¼: ä½(<{q25:.0f}), ä¸­({q25:.0f}~{q75:.0f}), é«˜(>{q75:.0f})")
print(f"çƒ­åº¦åˆ†å¸ƒ: {np.unique(hotness_labels, return_counts=True)}")

# ========================
# 3. æ•°æ®é¢„å¤„ç†
# ========================
# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_enhanced)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y, shuffle=True
)

# ========================
# 4. æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨é¢„è®¾çš„æœ€ä½³å‚æ•°ï¼Œé¿å…é•¿æ—¶é—´æœç´¢ï¼‰
# ========================
try:
    from xgboost import XGBClassifier
    
    print("ğŸš€ ä½¿ç”¨é¢„ä¼˜åŒ–çš„ XGBoost æ¨¡å‹...")
    
    # ä½¿ç”¨ç»éªŒæ€§è¾ƒå¥½çš„å‚æ•°ï¼ˆé¿å…é•¿æ—¶é—´æœç´¢ï¼‰
    classifier = XGBClassifier(
        n_estimators=300,
        max_depth=10,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.1,
        random_state=42,
        objective='multi:softprob',
        eval_metric='mlogloss',
        n_jobs=1  # è®¾ç½®ä¸º1ï¼Œé¿å…å¹¶è¡Œé—®é¢˜
    )
    
    print("å¼€å§‹è®­ç»ƒ XGBoost åˆ†ç±»å™¨...")
    classifier.fit(X_train, y_train)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    
    model_name = 'XGBoost_PreOptimized'
    
except ImportError:
    print("âŒ æœªå®‰è£… xgboostï¼Œä½¿ç”¨ä¼˜åŒ–çš„éšæœºæ£®æ—")
    from sklearn.ensemble import RandomForestClassifier
    
    classifier = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        random_state=42,
        n_jobs=1  # è®¾ç½®ä¸º1ï¼Œé¿å…å¹¶è¡Œé—®é¢˜
    )
    
    print("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨...")
    classifier.fit(X_train, y_train)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    
    model_name = 'RandomForest_PreOptimized'

# ========================
# 5. è¯„ä¼°æ¨¡å‹
# ========================
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')

print(f"\n=== æ¨¡å‹è¯„ä¼°ç»“æœ ({model_name}) ===")
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"F1å®å¹³å‡: {f1_macro:.4f}")

target_names = label_encoder.classes_
print("\n=== è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ===")
print(classification_report(y_test, y_pred, target_names=target_names))

cm = confusion_matrix(y_test, y_pred)
print("\n=== æ··æ·†çŸ©é˜µ ===")
print(cm)

# ========================
# 6. å¯è§†åŒ–ç»“æœ
# ========================
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.title(f'{model_name} æ··æ·†çŸ©é˜µ')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.tight_layout()
plt.show()

# ========================
# 7. ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
# ========================
joblib.dump(classifier, f'bilibili_classifier_{model_name.lower()}.pkl')
joblib.dump(label_encoder, 'label_encoder.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜ä¸º 'bilibili_classifier_{model_name.lower()}.pkl'")
print("âœ… æ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜ä¸º 'label_encoder.pkl'")
print("âœ… ç‰¹å¾æ ‡å‡†åŒ–å™¨å·²ä¿å­˜ä¸º 'feature_scaler.pkl'")

# ========================
# 8. ç¤ºä¾‹é¢„æµ‹å’Œæ¦‚ç‡åˆ†æ
# ========================
print("\n=== ç¤ºä¾‹é¢„æµ‹ ===")
y_pred_proba = classifier.predict_proba(X_test)
sample_idx = np.random.choice(len(X_test), 5, replace=False)

for i, idx in enumerate(sample_idx):
    true_label = label_encoder.inverse_transform([y_test[idx]])[0]
    pred_label = label_encoder.inverse_transform([y_pred[idx]])[0]
    prob = y_pred_proba[idx]
    print(f"æ ·æœ¬ {i+1}: çœŸå®={true_label}, é¢„æµ‹={pred_label}, "
          f"æ¦‚ç‡=[ä½:{prob[0]:.2f}, ä¸­:{prob[1]:.2f}, é«˜:{prob[2]:.2f}]")

# ========================
# 9. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
# ========================
try:
    # XGBoost ç‰¹å¾é‡è¦æ€§
    feature_importance = classifier.feature_importances_
    top_features_idx = np.argsort(feature_importance)[-10:]  # å‰10ä¸ªé‡è¦ç‰¹å¾
    top_importance = feature_importance[top_features_idx]
    
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(top_importance)), top_importance)
    plt.yticks(range(len(top_importance)), [f'Feature_{i}' for i in top_features_idx])
    plt.xlabel('é‡è¦æ€§')
    plt.title(f'{model_name} å‰10ä¸ªé‡è¦ç‰¹å¾')
    plt.tight_layout()
    plt.show()
    
    print(f"\n=== å‰5ä¸ªé‡è¦ç‰¹å¾ç´¢å¼• ===")
    for i in range(5):
        idx = top_features_idx[-(i+1)]
        imp = top_importance[-(i+1)]
        print(f"ç‰¹å¾ {idx}: é‡è¦æ€§ = {imp:.4f}")
        
except AttributeError:
    print("å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")

# ========================
# 10. é¢å¤–è¯„ä¼°æŒ‡æ ‡
# ========================
from sklearn.metrics import precision_recall_fscore_support

precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)
print("\n=== å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ ===")
for i, class_name in enumerate(target_names):
    print(f"{class_name}: P={precision[i]:.3f}, R={recall[i]:.3f}, F1={f1[i]:.3f}, Support={support[i]}")

# ========================
# 11. äº¤å‰éªŒè¯è¯„ä¼°ï¼ˆä½¿ç”¨å•è¿›ç¨‹ï¼‰
# ========================
print("\nğŸ” è¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°...")
cv_scores = cross_val_score(
    classifier, 
    X_scaled, y, 
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1_macro',
    n_jobs=1  # å•è¿›ç¨‹ï¼Œé¿å…ç¼–ç é—®é¢˜
)

print(f"5æŠ˜äº¤å‰éªŒè¯F1å®å¹³å‡åˆ†æ•°: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")